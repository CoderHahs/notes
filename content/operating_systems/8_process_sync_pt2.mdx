---
title: 'Process Synchronization Part 2'
metaTitle: 'Process Synchronization Part 2'
metaDescription: 'Operating Systems - Process Synchronization Part 2'
---

# Mutex Locks

The hardware solutions above are difficult for ordinary programmers to access, particularly on multi-processor machines, and particularly because they are often platform-dependent. Therefore most systems offer a software API equivalent called **mutex locks** or simply **mutexes**. (For mutual exclusion). The terminology is to *acquire* a lock prior to entering a critical section, and to *release* it when exiting.

Here is a solution to the critical section problem using mutex locks.

```C 
do {
    // acquire lock
        
        critical section

    // release lock

        remainder section

} while (TRUE);
```

Just as with hardware locks, the acquire step will block the process if the lock is in use by another process, and both the acquire and release operations are atomic. 

Below is how acquire and release are implemented based on a boolean variable `available`.

```C 
// Acquire
acquire() {
    while (!available)
        ; /* busy wait */
    available = false;;
}

// Release
release() {
    available = true;
}
```

One problem with the implementation shown here, (and in the hardware solutions presented earlier), is the busy loop used to block processes in the acquire phase. These types of locks are referred to as **spinlocks**, because the CPU just sits and spins while blocking the process. Spinlocks are very inefficient and waste CPU cycles on single-CPU single-threaded machines. However, they are good for multi-threaded machines.

# Semaphores

A more robust alternative to simple mutexes is to use **semaphores**, which are integer variables for which only two (atomic) operations are defined, the wait and signal operations, as shown in the following figure.

Semaphores are comprised of two operations shown below:

```C
// Wait
wait (S) {
    while S <= 0
        ; // no-op
    S--;
}

// Signal
signal (S) {
    S++;
}
```

## Semaphore Usage

In practice, semaphores can take on one of two forms:
- **Binary semaphores** can take on of two values, 0 or 1. They can be used to solve the critical section problem as described above, and can be used as mutexes on systems that do not provide a separate mutex mechanism. The general structure is shown below:

```C
do {
    waiting(mutex);
        
        // critical section

    signal (mutex);

        // remainder section

} while (TRUE);
```
- **Counting semaphores** can take on any integer value, and are usually used to count the number remaining of some limited resource. The counter is initialized to the number of such resources available in the system, and whenever the counting semaphore is greater than zero, then a process can enter a critical section and use one of the resources. When the counter gets to zero (or negative in some implementations), then the process blocks until another process frees up a resource and increments the counting semaphore with a signal call. 

Semaphores can also be used to synchronize certain operations between processes. For examples, suppose it is import that process P1 executes statement S1 before process P2 executes statement S2.

1. First we create a semaphore named `synch` that is shared by the two processes and initialize it as zero.
2. Then in process P1 we insert the code:
    ```C 
    S1;
    signal (synch);
    ```
3. and in process P2 we insert the code:
    ```C 
    wait (synch);
    S2;
    ```
4. Because `synch` was initialized to 0, process P2 will block on the wait until after P1 executes the call to signal.

## Semaphore implementation

The big problem with semaphores as described above is the busy loop in the wait call, which consumes CPU cycles with doing any useful work. 

An alternative approach is to block a process when it is forced to wait for an available semaphore, and sway it out of the CPU. In this implementation each semaphore needs to maintain a list of processes that are blocked waiting for it, so that one of the processes can be woken up and swapped back in when the semaphore becomes available. 

The new definition of a semaphore is as follows:
```C 
// Semaphore Structure:
typedef struct {
    int value;
    struct process *list;
} semaphore;

// Wait Operation:
wait (semaphore *S) {
    S -> value--;
    if (S -> value < 0) {
        add this process to S -> list;
        block();
    }
}

// Signal Operation:
signal(semaphore *S) {
    S -> value++;
    if (S -> value <= 0) {
        remove a process P from S->list;
        wakeup(P);
    }
}
```

## Deadlocks and starvation

One important problem that can arise when using semaphores to block processes waiting for a limited resource is the problem of **deadlocks**, which occur when multiple processes are blocked, each waiting for a resource that can only be freed by one of the other ( blocked ) processes.

![Image](https://www.cs.uic.edu/~jbell/CourseNotes/OperatingSystems/images/Chapter5/5_Deadlocks.jpg)

Another problem to consider is that of **starvation**, in which one or more processes gets blocked forever, and never get a chance to take their turn in the critical section. For example, in the semaphores above, we did not specify the algorithms for adding processes to the waiting queue in the semaphore in the wait( ) call, or selecting one to be removed from the queue in the signal( ) call. If the method chosen is a FIFO queue, then every process will eventually get their turn, but if a LIFO queue is implemented instead, then the first process to start waiting could starve.

## Priority Inversion

A challenging scheduling problem arises when a high-priority process gets blocked waiting for a resource that is currently held by a low-priority process.

If the low-priority process gets pre-empted by one or more medium-priority processes, then the high-priority process is essentially made to wait for the medium priority processes to finish before the low-priority process can release the needed resource, causing a **priority inversion**. If there are enough medium-priority processes, then the high-priority process may be forced to wait for a very long time.

One solution is a **priority-inheritance protocol**, in which a low-priority process holding a resource for which a high-priority process is waiting will temporarily inherit the high priority from the waiting process. This prevents the medium-priority processes from preempting the low-priority process until it releases the resource, blocking the priority inversion problem.